{"cells":[{"source":"![servicedesk](servicedesk.png)\n\nCleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.","metadata":{"executionCancelledAt":null,"executionTime":165,"lastExecutedAt":1707667023665,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."},"id":"e5870ae0-6165-459e-9c40-0f282883be7b","cell_type":"markdown"},{"source":"!pip install torchmetrics","metadata":{"executionCancelledAt":null,"executionTime":6691,"lastExecutedAt":1720759471862,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89"},"id":"0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting torchmetrics\n  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nCollecting lightning-utilities>=0.8.0 (from torchmetrics)\n  Downloading lightning_utilities-0.11.3.post0-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\nDownloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\nInstalling collected packages: lightning-utilities, torchmetrics\nSuccessfully installed lightning-utilities-0.11.3.post0 torchmetrics-1.4.0.post0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"}]},{"source":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\n\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":7065,"lastExecutedAt":1720759478928,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\n\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall","lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89"},"id":"2fa90b61-0244-4236-aa93-e33a7a088eec","cell_type":"code","execution_count":3,"outputs":[]},{"source":"nltk.download('punkt')","metadata":{"executionCancelledAt":null,"executionTime":279,"lastExecutedAt":1720759479208,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nltk.download('punkt')","outputsMetadata":{"0":{"height":80,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89"},"id":"37a51a81-1301-4a80-b8c6-716faaff4c5c","cell_type":"code","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":4}]},{"source":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')","metadata":{"executionCancelledAt":null,"executionTime":175,"lastExecutedAt":1720759479383,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')"},"id":"e1b12eaf-e55c-422c-94a2-b0197c465a1b","cell_type":"code","execution_count":5,"outputs":[]},{"source":"for i in text[:1]:\n    print('xxxxx')\n    print(i)","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1720759479431,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"for i in text[:1]:\n    print('xxxxx')\n    print(i)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"cell_type":"code","id":"c2b5a633-71ac-4a8d-bd39-a670afedca20","outputs":[{"output_type":"stream","name":"stdout","text":"xxxxx\n['i', 'called', 'because', 'i', 'have', 'been', 'receiving', '7', 'to', '8', 'calls', 'a', 'day', 'from', 'them', 'regarding', 'a', 'debt', 'and', 'the', 'representative', 'called', 'me', 'a', 'liar', 'after', 'i', 'asked', 'about', 'settling', 'my', 'account']\n"}],"execution_count":6},{"source":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)","metadata":{"executionCancelledAt":null,"executionTime":257,"lastExecutedAt":1720759479688,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)"},"id":"d630badb-23dd-4368-9a96-e2b478ad5cff","cell_type":"code","execution_count":7,"outputs":[]},{"source":"text.shape","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1720759479783,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"text.shape"},"cell_type":"code","id":"33a9c201-51b9-4a5f-8f91-325481024c52","outputs":[{"output_type":"execute_result","data":{"text/plain":"(5000, 50)"},"metadata":{},"execution_count":9}],"execution_count":9},{"source":"# from collections import Counter \n# Counter(labels)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1720759479831,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# from collections import Counter \n# Counter(labels)"},"cell_type":"code","id":"6cb308f2-5f7e-4358-a4ea-bb00b4d0f9e6","outputs":[],"execution_count":10},{"source":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1720759479883,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89"},"id":"f2654836-631f-415e-9922-5ab3bafaaafa","cell_type":"code","execution_count":11,"outputs":[]},{"source":"import torch.nn as nn\n\nclass RNN_classifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):\n        super(RNN_classifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 5)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.rnn(x, h0)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1720762082313,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch.nn as nn\n\nclass RNN_classifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):\n        super(RNN_classifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(embed_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 5)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        out, _ = self.rnn(x, h0)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out"},"cell_type":"code","id":"e1e973bc-af3b-43c7-9583-98fc8a8d9378","outputs":[],"execution_count":34},{"source":"print(torch.from_numpy(train_text).shape)\nprint(torch.from_numpy(train_text).unsqueeze(-1).shape)","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1720762015590,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(torch.from_numpy(train_text).shape)\nprint(torch.from_numpy(train_text).unsqueeze(-1).shape)","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"40b82a4c-b200-4c8f-bbae-70459ec37fac","outputs":[{"output_type":"stream","name":"stdout","text":"torch.Size([4000, 50])\ntorch.Size([4000, 50, 1])\n"}],"execution_count":32},{"source":"batch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n\n# Assuming `word2idx` and `train_data`, `test_data` are defined\nvocab_size = len(word2idx) + 1\nembed_dim = 64\nhidden_size = 6\nnum_layers = 1\nmodel = RNN_classifier(vocab_size, embed_dim, hidden_size, num_layers)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nmodel.train()\nfor i in range(10):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        model.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {i + 1}, Loss: {running_loss / num_processed}\")\n","metadata":{"executionCancelledAt":null,"executionTime":10666,"lastExecutedAt":1720762114378,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"batch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n\n# Assuming `word2idx` and `train_data`, `test_data` are defined\nvocab_size = len(word2idx) + 1\nembed_dim = 64\nhidden_size = 6\nnum_layers = 1\nmodel = RNN_classifier(vocab_size, embed_dim, hidden_size, num_layers)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nmodel.train()\nfor i in range(10):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        model.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {i + 1}, Loss: {running_loss / num_processed}\")\n","outputsMetadata":{"0":{"height":227,"type":"stream"}}},"cell_type":"code","id":"6770e746-c97c-44f5-8c72-36ac55729044","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, Loss: 0.004223566025495529\nEpoch: 2, Loss: 0.004164794117212296\nEpoch: 3, Loss: 0.00412275105714798\nEpoch: 4, Loss: 0.004089788258075714\nEpoch: 5, Loss: 0.004063237667083741\nEpoch: 6, Loss: 0.004040512889623642\nEpoch: 7, Loss: 0.004020933121442795\nEpoch: 8, Loss: 0.004004344373941421\nEpoch: 9, Loss: 0.003989219009876251\nEpoch: 10, Loss: 0.003976047515869141\n"}],"execution_count":36},{"source":"# Evaluate model on test set\nmodel.eval()\naccuracy_metric = Accuracy(task='multiclass', num_classes=5)\nprecision_metric = Precision(task='multiclass', num_classes=5, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=5, average=None)\n\nfor inputs, labels in test_loader:\n    with torch.no_grad():\n        output = model(inputs)\n        cat = torch.argmax(output, dim=-1)\n        accuracy_metric(cat, labels)\n        precision_metric(cat, labels)\n        recall_metric(cat, labels)\n\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","metadata":{"executionCancelledAt":null,"executionTime":131,"lastExecutedAt":1720762161861,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Evaluate model on test set\nmodel.eval()\naccuracy_metric = Accuracy(task='multiclass', num_classes=5)\nprecision_metric = Precision(task='multiclass', num_classes=5, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=5, average=None)\n\nfor inputs, labels in test_loader:\n    with torch.no_grad():\n        output = model(inputs)\n        cat = torch.argmax(output, dim=-1)\n        accuracy_metric(cat, labels)\n        precision_metric(cat, labels)\n        recall_metric(cat, labels)\n\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"c9d7668f-b182-4cb2-87e1-72588b022de0","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.20600000023841858\nPrecision (per class): [0.1726190447807312, 0.20608898997306824, 0.1666666716337204, 0.2460317462682724, 0.20879121124744415]\nRecall (per class): [0.1510416716337204, 0.46315789222717285, 0.004629629664123058, 0.1614583283662796, 0.27142858505249023]\n"}],"execution_count":38},{"source":"# Define the classifier class\nclass TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim=2) \n        return self.fc(conved)\n\n\nvocab_size = len(word2idx) + 1\ntarget_size = len(np.unique(labels))\nembedding_dim = 64\n\n# Create an instance of the TicketClassifier class\nmodel = TicketClassifier(vocab_size, embedding_dim, target_size)\n\nlr = 0.05\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nepochs = 3\n\n# Train the model\nmodel.train()\nfor i in range(epochs):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        model.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {i+1}, Loss: {running_loss/num_processed}\")\n\n\naccuracy_metric = Accuracy(task='multiclass', num_classes=5)\nprecision_metric = Precision(task='multiclass', num_classes=5, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=5, average=None)\n\n# Evaluate model on test set\nmodel.eval()\npredicted = []\n\nfor i, (inputs, labels) in enumerate(test_loader):\n    output = model(inputs)\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    accuracy_metric(cat, labels)\n    precision_metric(cat, labels)\n    recall_metric(cat, labels)\n\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","metadata":{"executionCancelledAt":null,"executionTime":2425,"lastExecutedAt":1720760454079,"lastExecutedByKernel":"e86fa375-9bc8-49fe-8615-f4a62954da89","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the classifier class\nclass TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)\n        conved = F.relu(self.conv(embedded))\n        conved = conved.mean(dim=2) \n        return self.fc(conved)\n\n\nvocab_size = len(word2idx) + 1\ntarget_size = len(np.unique(labels))\nembedding_dim = 64\n\n# Create an instance of the TicketClassifier class\nmodel = TicketClassifier(vocab_size, embedding_dim, target_size)\n\nlr = 0.05\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nepochs = 3\n\n# Train the model\nmodel.train()\nfor i in range(epochs):\n    running_loss, num_processed = 0,0\n    for inputs, labels in train_loader:\n        model.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_processed += len(inputs)\n    print(f\"Epoch: {i+1}, Loss: {running_loss/num_processed}\")\n\n\naccuracy_metric = Accuracy(task='multiclass', num_classes=5)\nprecision_metric = Precision(task='multiclass', num_classes=5, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=5, average=None)\n\n# Evaluate model on test set\nmodel.eval()\npredicted = []\n\nfor i, (inputs, labels) in enumerate(test_loader):\n    output = model(inputs)\n    cat = torch.argmax(output, dim=-1)\n    predicted.extend(cat.tolist())\n    accuracy_metric(cat, labels)\n    precision_metric(cat, labels)\n    recall_metric(cat, labels)\n\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\nprint('Accuracy:', accuracy)\nprint('Precision (per class):', precision)\nprint('Recall (per class):', recall)","outputsMetadata":{"0":{"height":143,"type":"stream"}}},"cell_type":"code","id":"8fd8e53a-f0bf-4c41-8733-f2967524dfc7","outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, Loss: 0.0039164722561836245\nEpoch: 2, Loss: 0.0018052333444356918\nEpoch: 3, Loss: 0.0007527070641517639\nAccuracy: 0.8019999861717224\nPrecision (per class): [0.7388888597488403, 0.7268292903900146, 0.8095238208770752, 0.849397599697113, 0.8807339668273926]\nRecall (per class): [0.6927083134651184, 0.7842105031013489, 0.8657407164573669, 0.734375, 0.9142857193946838]\n"}],"execution_count":24}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
