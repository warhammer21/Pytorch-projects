{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport torch\nimport torch.nn as nn\n\n# Define possible layer types\nlayer_types = ['Linear', 'Conv1d', 'Conv2d', 'LSTM', 'GRU']\n\ndef generate_layer(layer_type, last_output_shape=None):\n    if layer_type == 'Linear':\n        if last_output_shape is not None:\n            in_features = last_output_shape[-1]\n        else:\n            in_features = random.randint(10, 100)\n        out_features = random.randint(10, 100)\n        return f\"nn.Linear({in_features}, {out_features})\", [out_features]\n\n    if layer_type == 'Conv1d':\n        if last_output_shape is not None and len(last_output_shape) == 2:\n            in_channels = last_output_shape[0]\n            length = last_output_shape[1]\n        else:\n            in_channels = random.randint(10, 100)\n            length = random.randint(50, 100)\n        out_channels = random.randint(1, 10)\n        kernel_size = random.randint(1, 5)\n        new_length = length - kernel_size + 1  # Simplified length calculation for example\n        return f\"nn.Conv1d({in_channels}, {out_channels}, kernel_size={kernel_size})\", [out_channels, new_length]\n\n    if layer_type == 'Conv2d':\n        if last_output_shape is not None and len(last_output_shape) == 3:\n            in_channels = last_output_shape[0]\n            height = last_output_shape[1]\n            width = last_output_shape[2]\n        else:\n            in_channels = random.randint(10, 100)\n            height = width = random.randint(50, 100)\n        out_channels = random.randint(1, 10)\n        kernel_size = random.randint(1, 5)\n        new_height = height - kernel_size + 1  # Simplified height calculation for example\n        new_width = width - kernel_size + 1  # Simplified width calculation for example\n        return f\"nn.Conv2d({in_channels}, {out_channels}, kernel_size={kernel_size})\", [out_channels, new_height, new_width]\n\n    if layer_type in ['LSTM', 'GRU']:\n        if last_output_shape is not None and len(last_output_shape) > 0:\n            input_size = last_output_shape[-1]\n            seq_len = last_output_shape[0]\n        else:\n            input_size = random.randint(10, 100)\n            seq_len = random.randint(1, 10)\n        hidden_size = random.randint(10, 100)\n        return f\"nn.{layer_type}({input_size}, {hidden_size})\", [seq_len, hidden_size]\n\ndef generate_synthetic_data(num_samples):\n    data = []\n    for _ in range(num_samples):\n        num_layers = random.randint(1, 5)\n        architecture = []\n        last_output_shape = None\n\n        for _ in range(num_layers):\n            layer_type = random.choice(layer_types)\n            layer, output_shape = generate_layer(layer_type, last_output_shape)\n            architecture.append(layer)\n            last_output_shape = output_shape\n\n        architecture_str = \"\\n\".join(architecture)\n        input_shape_str = f\"[{', '.join(map(str, architecture[0].split('(')[1].split(')')[0].split(', ')))}]\"\n        output_shape_str = f\"[{', '.join(map(str, last_output_shape))}]\"\n        description = f\"Input shape: {input_shape_str}, Output shape: {output_shape_str}\"\n        data.append((architecture_str, description))\n\n    return data\n\n# Parameters\nnum_samples = 60000\n\n# Generate synthetic data\nsynthetic_data = generate_synthetic_data(num_samples)\n\n# Display generated synthetic data\nfor i in range(5):\n    print(f\"Network Architecture:\\n{synthetic_data[i][0]}\\nDescription: {synthetic_data[i][1]}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:13:14.033554Z","iopub.execute_input":"2024-07-16T20:13:14.033964Z","iopub.status.idle":"2024-07-16T20:13:17.636815Z","shell.execute_reply.started":"2024-07-16T20:13:14.033932Z","shell.execute_reply":"2024-07-16T20:13:17.635622Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Network Architecture:\nnn.Linear(57, 16)\nnn.Conv1d(19, 1, kernel_size=5)\nnn.Linear(73, 67)\nDescription: Input shape: [57, 16], Output shape: [67]\n\nNetwork Architecture:\nnn.LSTM(47, 96)\nnn.Conv2d(61, 9, kernel_size=1)\nnn.LSTM(60, 100)\nnn.LSTM(100, 82)\nnn.LSTM(82, 43)\nDescription: Input shape: [47, 96], Output shape: [9, 43]\n\nNetwork Architecture:\nnn.LSTM(12, 49)\nDescription: Input shape: [12, 49], Output shape: [1, 49]\n\nNetwork Architecture:\nnn.GRU(42, 52)\nnn.GRU(52, 49)\nnn.GRU(49, 41)\nnn.GRU(41, 69)\nnn.GRU(69, 72)\nDescription: Input shape: [42, 52], Output shape: [6, 72]\n\nNetwork Architecture:\nnn.Conv2d(64, 1, kernel_size=1)\nnn.Conv2d(1, 6, kernel_size=4)\nnn.Conv2d(6, 8, kernel_size=3)\nnn.Conv1d(78, 5, kernel_size=3)\nDescription: Input shape: [64, 1, kernel_size=1], Output shape: [5, 74]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torch.utils.data import DataLoader, Dataset\n\n# Assuming synthetic_data and vocab are already defined from your previous steps\n\ntokenizer = get_tokenizer('basic_english')\n\n# Function to yield tokens from both architectures and descriptions\ndef yield_tokens(data):\n    for architecture, description in data:\n        yield tokenizer(architecture)\n        yield tokenizer(description)\n\n# Build vocabulary from both architectures and descriptions\nvocab = build_vocab_from_iterator(yield_tokens(synthetic_data), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# Function to process data into tokenized tensors\ndef process_data(data, vocab):\n    processed_data = []\n    for architecture, description in data:\n        arch_tensor = torch.tensor([vocab[\"<bos>\"]] + [vocab[token] for token in tokenizer(architecture)] + [vocab[\"<eos>\"]], dtype=torch.long)\n        desc_tensor = torch.tensor([vocab[\"<bos>\"]] + [vocab[token] for token in tokenizer(description)] + [vocab[\"<eos>\"]], dtype=torch.long)\n        processed_data.append((arch_tensor, desc_tensor))\n    return processed_data\n\n# Split data into train, validation, and test sets\ntrain_data = process_data(synthetic_data[:55000], vocab)\n#valid_data = process_data(synthetic_data[8000:9000], vocab)\ntest_data = process_data(synthetic_data[56000:], vocab)\n\n# # Creating DataLoaders\n# train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n# valid_loader = DataLoader(valid_data, batch_size=1, shuffle=False)\n# test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n\n# Reverse vocabulary for decoding\nreverse_vocab = {index: token for token, index in vocab.get_stoi().items()}\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:13:17.639031Z","iopub.execute_input":"2024-07-16T20:13:17.639572Z","iopub.status.idle":"2024-07-16T20:13:28.719414Z","shell.execute_reply.started":"2024-07-16T20:13:17.639539Z","shell.execute_reply":"2024-07-16T20:13:28.718242Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"for i in train_data[:3]:\n    print(i[0].shape,i[1].shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:13:28.720887Z","iopub.execute_input":"2024-07-16T20:13:28.721470Z","iopub.status.idle":"2024-07-16T20:13:28.727116Z","shell.execute_reply.started":"2024-07-16T20:13:28.721436Z","shell.execute_reply":"2024-07-16T20:13:28.726036Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"torch.Size([28]) torch.Size([11])\ntorch.Size([44]) torch.Size([13])\ntorch.Size([10]) torch.Size([13])\n","output_type":"stream"}]},{"cell_type":"code","source":"len(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:13:28.728958Z","iopub.execute_input":"2024-07-16T20:13:28.729887Z","iopub.status.idle":"2024-07-16T20:13:28.740160Z","shell.execute_reply.started":"2024-07-16T20:13:28.729842Z","shell.execute_reply":"2024-07-16T20:13:28.739129Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"416"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n# Define the Encoder\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n        \n    def forward(self, src):\n        embedded = self.embedding(src)\n        outputs, hidden = self.rnn(embedded)\n        return hidden\n\n# Define the Decoder\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, input, hidden):\n        input = input.unsqueeze(1)  # Add time dimension\n        embedded = self.embedding(input)\n        output, hidden = self.rnn(embedded, hidden)\n        prediction = self.fc_out(output.squeeze(1))\n        return prediction, hidden\n\n# Define the Seq2Seq Model\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.fc_out.out_features\n        \n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(trg.device)\n        hidden = self.encoder(src)\n        \n        input = trg[:, 0]\n        \n        for t in range(1, trg_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[:, t, :] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[:, t] if teacher_force else top1\n        \n        return outputs\n\n# Custom Dataset\nclass TranslationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Dummy data\nvocab_size = len(vocab)  # Example vocabulary size\ninput_dim = vocab_size\noutput_dim = vocab_size\nemb_dim = 10\nhidden_dim = 20\n\n# Create encoder, decoder, and Seq2Seq model\nencoder = Encoder(input_dim, emb_dim, hidden_dim)\ndecoder = Decoder(output_dim, emb_dim, hidden_dim)\nmodel = Seq2Seq(encoder, decoder)\n\n# Define optimizer and loss function\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n\n# Sample train data\n# train_data = [\n#     (torch.randint(1, vocab_size, (28,)), torch.randint(1, vocab_size, (15,))),\n#     (torch.randint(1, vocab_size, (20,)), torch.randint(1, vocab_size, (13,))),\n#     (torch.randint(1, vocab_size, (12,)), torch.randint(1, vocab_size, (17,)))\n# ]\n\n# Create DataLoader\nbatch_size = 800\ntrain_dataset = TranslationDataset(train_data)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: zip(*x))\n\n# Training loop\nfor epoch in range(19):  # Number of epochs\n    for batch in train_loader:\n        src_batch, trg_batch = batch\n        src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0).T\n        trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=0).T\n\n        # Ensure trg is LongTensor\n        trg_batch = trg_batch.long()\n\n        optimizer.zero_grad()\n        \n        output = model(src_batch, trg_batch)\n        \n        # Reshape for loss calculation\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg_batch = trg_batch[:, 1:].reshape(-1)\n\n        loss = criterion(output, trg_batch)\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n\n# Verifying output shape for one sample batch\nsrc_batch, trg_batch = next(iter(train_loader))\nsrc_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0).T\ntrg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=0).T\noutput = model(src_batch, trg_batch)\nprint(\"Output shape:\", output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:13:33.136464Z","iopub.execute_input":"2024-07-16T20:13:33.136967Z","iopub.status.idle":"2024-07-16T20:18:06.736325Z","shell.execute_reply.started":"2024-07-16T20:13:33.136933Z","shell.execute_reply":"2024-07-16T20:18:06.734819Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 4.515774726867676\nEpoch 2, Loss: 3.352123975753784\nEpoch 3, Loss: 3.1667118072509766\nEpoch 4, Loss: 2.9304709434509277\nEpoch 5, Loss: 2.7308504581451416\nEpoch 6, Loss: 2.5927176475524902\nEpoch 7, Loss: 2.2517282962799072\nEpoch 8, Loss: 2.152487277984619\nEpoch 9, Loss: 2.0454771518707275\nEpoch 10, Loss: 2.007946491241455\nEpoch 11, Loss: 1.9130191802978516\nEpoch 12, Loss: 1.976943016052246\nEpoch 13, Loss: 1.8415275812149048\nEpoch 14, Loss: 1.7984631061553955\nEpoch 15, Loss: 1.7742600440979004\nEpoch 16, Loss: 1.7413744926452637\nEpoch 17, Loss: 1.7583849430084229\nEpoch 18, Loss: 1.6891485452651978\nEpoch 19, Loss: 1.643355369567871\nOutput shape: torch.Size([800, 17, 416])\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:18:12.343182Z","iopub.execute_input":"2024-07-16T20:18:12.343804Z","iopub.status.idle":"2024-07-16T20:18:12.351439Z","shell.execute_reply.started":"2024-07-16T20:18:12.343770Z","shell.execute_reply":"2024-07-16T20:18:12.350111Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(416, 10)\n    (rnn): RNN(10, 20, batch_first=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(416, 10)\n    (rnn): RNN(10, 20, batch_first=True)\n    (fc_out): Linear(in_features=20, out_features=416, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# Sample test data, same format as train_data\n# test_data = [\n#     (torch.randint(1, vocab_size, (28,)), torch.randint(1, vocab_size, (15,))),\n#     (torch.randint(1, vocab_size, (20,)), torch.randint(1, vocab_size, (13,))),\n#     (torch.randint(1, vocab_size, (12,)), torch.randint(1, vocab_size, (17,)))\n# ]\n\ntest_dataset = TranslationDataset(test_data)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: zip(*x))\n\n\n\ndef evaluate(model, data_loader, criterion):\n    model.eval()\n    epoch_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            src_batch, trg_batch = batch\n            src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=0).T\n            trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=0).T\n\n            trg_batch = trg_batch.long()\n\n            output = model(src_batch, trg_batch, teacher_forcing_ratio=0)\n            \n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            trg_batch = trg_batch[:, 1:].reshape(-1)\n\n            loss = criterion(output, trg_batch)\n            epoch_loss += loss.item()\n\n            preds = output.argmax(1).cpu().numpy()\n            labels = trg_batch.cpu().numpy()\n\n            non_pad_elements = (labels != 0)\n            all_preds.extend(preds[non_pad_elements])\n            all_labels.extend(labels[non_pad_elements])\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return epoch_loss / len(data_loader), accuracy, f1\n\ntest_loss, test_accuracy, test_f1 = evaluate(model, test_loader, criterion)\n\nprint(f'Test Loss: {test_loss:.3f}')\nprint(f'Test Accuracy: {test_accuracy:.3f}')\nprint(f'Test F1 Score: {test_f1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:18:19.470101Z","iopub.execute_input":"2024-07-16T20:18:19.471322Z","iopub.status.idle":"2024-07-16T20:18:20.569496Z","shell.execute_reply.started":"2024-07-16T20:18:19.471262Z","shell.execute_reply":"2024-07-16T20:18:20.568315Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Test Loss: 1.910\nTest Accuracy: 0.547\nTest F1 Score: 0.501\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n\n# Function to decode tensors back to strings\ndef decode_tensor(tensor, reverse_vocab):\n    tokens = [reverse_vocab[idx.item()] for idx in tensor if idx.item() in reverse_vocab]\n    return ' '.join(tokens)\n\n# Evaluation loop with decoding\nmodel.eval()\nwith torch.no_grad():\n    for i, (src, trg) in enumerate(test_loader):\n        if i >= 10:  # Limit to the top 10 batches\n            break\n        src, trg = src.cpu(), trg.cpu()  # Move tensors to CPU\n        \n        # Forward pass\n        output = model(src, trg, 0)  # Turn off teacher forcing\n        \n        # Get the top predicted token from each timestep\n        top_pred = output.argmax(2)\n        \n        # Decode tensors to strings\n        decoded_src = decode_tensor(src[0], reverse_vocab)\n        decoded_trg = decode_tensor(trg[0], reverse_vocab)\n        decoded_pred = decode_tensor(top_pred[0], reverse_vocab)\n        \n        print(f\"Source: {decoded_src}\")\n        print(f\"Target: {decoded_trg}\")\n        print(f\"Prediction: {decoded_pred}\")\n        print(\"=\"*50)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T20:29:01.811191Z","iopub.execute_input":"2024-07-16T20:29:01.811600Z","iopub.status.idle":"2024-07-16T20:29:01.871122Z","shell.execute_reply.started":"2024-07-16T20:29:01.811569Z","shell.execute_reply":"2024-07-16T20:29:01.869978Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Source: <bos> nn . gru ( 57 , 85 ) nn . gru ( 85 , 11 ) nn . gru ( 11 , 44 ) nn . conv2d ( 22 , 9 , kernel_size=1 ) <eos>\nTarget: <bos> input shape [57 , 85] , output shape [9 , 66 , 66] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos>\n==================================================\nSource: <bos> nn . conv1d ( 88 , 9 , kernel_size=3 ) nn . gru ( 71 , 84 ) nn . linear ( 84 , 55 ) nn . lstm ( 55 , 75 ) nn . conv2d ( 70 , 1 , kernel_size=4 ) <eos>\nTarget: <bos> input shape [88 , 9 , kernel_size=3] , output shape [1 , 75 , 75] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos> output <eos>\n==================================================\nSource: <bos> nn . gru ( 15 , 82 ) nn . lstm ( 82 , 95 ) nn . linear ( 95 , 67 ) nn . conv1d ( 49 , 3 , kernel_size=4 ) nn . conv1d ( 3 , 10 , kernel_size=4 ) <eos>\nTarget: <bos> input shape [15 , 82] , output shape [10 , 64] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos>\n==================================================\nSource: <bos> nn . conv1d ( 24 , 2 , kernel_size=4 ) <eos>\nTarget: <bos> input shape [24 , 2 , kernel_size=4] , output shape [2 , 73] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos>\n==================================================\nSource: <bos> nn . lstm ( 91 , 22 ) <eos>\nTarget: <bos> input shape [91 , 22] , output shape [6 , 22] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos>\n==================================================\nSource: <bos> nn . linear ( 13 , 89 ) nn . conv1d ( 84 , 8 , kernel_size=4 ) nn . conv1d ( 8 , 8 , kernel_size=2 ) nn . lstm ( 62 , 17 ) nn . conv1d ( 8 , 10 , kernel_size=4 ) <eos>\nTarget: <bos> input shape [13 , 89] , output shape [10 , 14] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos>\n==================================================\nSource: <bos> nn . conv1d ( 40 , 8 , kernel_size=4 ) nn . conv1d ( 8 , 3 , kernel_size=3 ) <eos>\nTarget: <bos> input shape [40 , 8 , kernel_size=4] , output shape [3 , 87] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos>\n==================================================\nSource: <bos> nn . conv1d ( 58 , 5 , kernel_size=1 ) nn . linear ( 71 , 30 ) nn . conv1d ( 46 , 7 , kernel_size=3 ) <eos>\nTarget: <bos> input shape [58 , 5 , kernel_size=1] , output shape [7 , 84] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos>\n==================================================\nSource: <bos> nn . conv1d ( 87 , 9 , kernel_size=3 ) nn . gru ( 81 , 80 ) <eos>\nTarget: <bos> input shape [87 , 9 , kernel_size=3] , output shape [9 , 80] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos>\n==================================================\nSource: <bos> nn . linear ( 84 , 12 ) nn . conv2d ( 11 , 2 , kernel_size=4 ) nn . linear ( 83 , 79 ) nn . conv1d ( 62 , 6 , kernel_size=4 ) nn . conv2d ( 80 , 2 , kernel_size=1 ) <eos>\nTarget: <bos> input shape [84 , 12] , output shape [2 , 93 , 93] <eos>\nPrediction: <unk> input shape [40 , 4 , output shape output , output <eos> output <eos>\n==================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}